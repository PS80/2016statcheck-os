{
    "contents" : "---\n---\ntitle: \"The prevalence of statistical inconsistencies in management research: A replication of Nuijten et al. (2015)\"\nshorttitle: statistical inconsistencies\nauthor: \n  - name: Johannes M. van Zelst\n    affiliation: 1\n    corresponding: yes    # Define only one corresponding author\n    email: j.m.vanzelst@uvt.nl\n    address: Warandelaan 2, 5037 AB Tilburg, The Netherlands\n  - name: Peter Kruyen\n    affiliation: 2\n    corresponding: no    # Define only one corresponding author\n    email: p.m.kruyen@fm.ru.nl\n    address: Thomas van Aquinostraat 5.1.43, 6525 GD Nijmegen, The Netherlands\n  - name: Chris H. J. Hartgerink\n    affiliation: 3\n    corresponding: no    # Define only one corresponding author\n    email: c.h.j.hartgerink@uvt.nl\n    address: Warandelaan 2, 5037 AB Tilburg, The Netherlands\naffiliation:\n  - id: 1\n    institution: Department of Organization Studies, Tilburg University\n  - id: 2\n    institution: Department of Public Administration, Radboud University Nijmegen\n  - id: 3\n    institution: Department of Methodology and Statistics, Tilburg University\n    \n\nabstract: \n  This study documents reporting errors in a sample of over 250,000 p-values reported in forty major management journals from 1995 until 2015, using the new R package statcheck.\n  \ndate: \"Wednesday, October 07, 2016\"\noutput: papaja::apa6_pdf\nstyle: apa.csl\nbibliography: statcheck.bib\n\nclass: man\nlang: english\nfigsintext: no\nfigurelist: no\ntablelist: no\nlineno: no\n\n---\n#### INTRODUCTION\nThe use of null hypothesis significance testing (NHST) is widespread in management and public administration research [@Lockett2014; @Orlitzky2012; @Schwab2011]. The verification and replicability of null hypotheses relies on correctly reported statistical results, such as test statistics, degrees of freedom and p-values. However, studies in scientific fields such as psychology and medicine show that papers too often report statistical inconsistencies where some even lead to a change in substantive conclusions [@Bakker2011; @Garcia-Berthou2004; @Nuijten2015]. Recently, management scholars started to document that some statistical findings might be inaccurate [@Goldfarb2016]. Management scholars also noted that “honest mistakes and possible scientific misconduct pose a worrisome threat to the trustworthiness of accumulated knowledge” [@Bergh2016a, p. 2] An assessment of the prevalence of statistical inconsistencies in the field of management and public administration is required to uphold the trustworthiness in our results and theories. We therefore conduct a replication of @Nuijten2015 and investigate the prevalence of statistical inconsistencies in 33 leading management and public administration journals over a time period of twenty years.  \nA statistical inconsistency can occur when either the test statistic, the degrees of freedom or the resulting p-value is erroneously reported. The misrepresentation of any of these values may have severe consequences for conclusions that are drawn with respect to the underlying theory. First, meta-analyses may produce biased results when erroneous test statistics or degrees of freedom, i.e. sample size, are reported in studies and subsequently used as data by meta-analysts. Second, when a p-value is interpreted as significant while it is actually insignificant, this may lead to wrong substantive conclusions. It is therefore important that authors, reviewers and editors spot these inconsistencies whenever possible. A documentation of the inconsistency rate in management and public administration is therefore necessary if we are to uphold the trustworthiness of these fields.  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The erroneous reporting of statistical results might be the result of accidental inconsistencies or purposefully rounding off to a statistically significant result. First, authors can mistakenly round of a p-value (for example, reporting a p-value of 0.056 as p = 0.05) or incorrectly report test statistics and/or sample sizes. For example, “a simple typographical error such as *F*(2,56) = 1.203, p < .001 instead of *F*(2,56) = 12.03, p < .001 produces a gross inconsistency, without the p-value being incorrect” [@Nuijten2015 p. 10]. On the other hand, authors sometimes also engage in intentionally rounding of a p-value to make the result come across as significant while it actually was insignificant. @Banks2016 find in a large-scale survey of management scholars that more than 10% of their respondents engaged in this form of questionable research practice in at least one study. Given the serious consequences of statistical inconsistencies, whether they are accidental or intentional, we attempt to systematically document the prevalence of statistical inconsistencies in the organizational sciences.  \nOur contributions are threefold. First, we document the prevalence of reporting inconsistencies in management and public administration research. The results can be directly compared to the study of @Nuijten2015 as we replicate their study using the same analyses. Second, we study whether inconsistencies have increased over time and if they are related to journal characteristics such as publisher and impact factor. We can draw population-level conclusions as we use an automated procedure (the R package statcheck, Epskamp & Nuijten, 2015) which allows us to extract test statistics from thousands of published articles. Last, we offer a number of solutions that might help to partly solve the problem of reporting errors for future research.\n\n#### METHODS  \n*Sample*  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A selection of articles was made by searching the flagship empirical journals in management and public administration research. We used a link spider and scraper to automatically download HTML articles from top-tier journals that publish empirical papers. For our main results, we excluded articles that were only available in PDF format as the reliability of these results are lower given our automated procedure (see below). We downloaded all articles that were available in HTML from 1995-2015 to provide a comprehensive and timely overview of reporting errors. Stuk over hoe we precies gedownload hebben.  We downloaded all articles in an automated fashion, so the sample of downloaded articles includes editorials, corrigendums and retractions. The final number of articles that we downloaded is X. Table 1 contains the publisher per journal, the number of articles that we downloaded per journal, and the number of articles from which we were able to extract results.  \n\n*Explaining statcheck*  \nTo document the prevalence of reporting errors on a large-scale, we made use of an automated procedure in R (R Core Team, 2016). Statcheck “extracts statistical results and recalculates p-values based on reported test statistics and their degrees of freedom” [@Nuijten2015, p. 2]. Statcheck executes the procedure in four steps. \nFirst, statcheck converts the collection of PDF or html files that are read by R into plain text files. PDF files are more problematic to convert as some publishers use signs instead of actual characters. Furthermore, some publishers place text in multiple columns per page, where test statistics, degrees of freedom and the associated p-value can be placed over multiple columns. The program is unable to deal with these issues when PDF files are used, whereas html files are rendered correctly.   \nSecond, statcheck extracts *t*, *F*, *r*, *χ2*, and *Z* statistics as well as the accompanying degrees of freedom and p-value. As statcheck is an automated procedure, it can only read prespecified strings of text. The program is currently capable of reading results that are reported in APA format, for example ‘*F*(1, 238) = 2.94, p = 0.088’.  Furthermore, “statcheck takes different spacing into account, and also reads results that are reported as nonsignificant (ns)” [@Nuijten2015, p. 2]. This implies however that the program is currently unable to read results that are reported in tables. Therefore, we are unable to assess the prevalence of errors in tables that report, for example, regression results. Third, statcheck recalculates a p-value on the basis of the reported test statistics and the associated degrees of freedom. Tests are assumed to be two-tailed by default, but the program also checks and reports whether the text mentions one-tailed tests.  \nFinally, statcheck compares the reported p-value with the recalculated p-value and indicates whether the reported p-value is consistent with the reported test statistic and degrees of freedom or not. In case the recalculated p-value and the reported value are not the same, the result is marked as a reporting error. Sometimes, a reporting error might actually change the conclusion of the associated test (under α = 0.05). Statcheck then reports the result as a decision error. For example, the reported p-value is ‘p < 0.05’ while the recalculated p-value is ‘p = 0.052’. As a robustness check, we also report the number of decision errors when α is set to 0.10 (commonly regarded as ‘marginal significance’ in management and public administration research).  Statcheck subsequently scans the text for the words “one-tailed”, “one-sided”, or “directional”. Results that were set to error/decision error are marked as consistent if the article mentions one of these words.  \nThe advantage of this automated procedure is that it allows us to assess the prevalence of reporting inconsistencies on a very large scale. Furthermore, the automated procedure eliminates human errors which are bound to be made when results are recalculated by hand. The disadvantage of an automated procedure is that it will miss statistical tests that are not reported according to APA standards. @Nuijten2015 conducted an extensive validity check to assess whether statcheck does not produce a biased assessments of the inconsistency rate. They compared a subsample of the results of statcheck’s analysis with a manual check of the same p-values and found that the inter-rater reliability for errors was 0.76 and for decision errors 0.89. Statcheck slightly overestimated the prevalence of errors in this validity check, so the results should be interpreted with care. For a detailed discussion of the validity of statcheck, see @Nuijten2015.  \n\n*Analyses*  \nThe population of interest here is all APA-reported test results in the selected top-tier journals in management and public administration research. Our sample thus includes the entire population of articles with results reported in APA-style, which is why we do not use statistical tests in our analyses as we do not need to generalize from our sample to a population. We report the prevalence of (gross) inconsistencies per journal and explore linear trends over time. We also compare whether gross inconsistencies are more likely for results that are reported as statistically significant as compared to insignificant results. Last, we analyze a number of journal-level covariates to observe whether they influence the number of inconsistencies. We explore whether there are differences in the percentage of inconsistencies across different publishers: we included journals from general publishers such as Wiley, Elsevier, and Sage as well as dedicated publishers INFORMS, the Academy of Management, and one APA journal. We also report regression results for the relationship between journal impact factor and the amount of inconsistencies. Previous research has found that journal impact factor and retraction frequency of articles are positively related [@Fang2011]. Therefore, it might be interesting to explore whether journal impact factor and the prevalence of statistical inconsistencies are related.  \n\n*Robustness*  \nSince many journals only publish HTML articles for more recent years, we also downloaded all articles in PDF and used statcheck on these articles. As explained above, the conversion to text files is less reliable for PDFs than for HTML files. We therefore use this robustness check as a sensitivity analysis and the results ought to be interpreted with caution.  \n\n#### References\n\n",
    "created" : 1475860353531.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3881746485",
    "id" : "F31895A3",
    "lastKnownWriteTime" : 1475863890,
    "path" : "C:/Users/Marino/Dropbox/projects/2016statcheck/data/statcheck.Rmd",
    "project_path" : "statcheck.Rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "type" : "r_markdown"
}