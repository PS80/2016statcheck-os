\documentclass[english,man]{apa6}

\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}

% Table formatting
\usepackage{longtable, booktabs}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
\newenvironment{ltable}
  {\begin{landscape}\begin{center}\begin{threeparttable}}
  {\end{threeparttable}\end{center}\end{landscape}}

\newenvironment{lltable}
  {\begin{landscape}\begin{center}\begin{ThreePartTable}}
  {\end{ThreePartTable}\end{center}\end{landscape}}

\usepackage{ifthen} % Only add declarations when endfloat package is loaded
\ifthenelse{\equal{man}{\string jou}}{%
  \DeclareDelayedFloatFlavor{ThreePartTable}{table} % Make endfloat play with longtable
  \DeclareDelayedFloatFlavor{ltable}{table} % Make endfloat play with lscape
  \DeclareDelayedFloatFlavor{lltable}{table} % Make endfloat play with lscape & longtable
}{}%


% The following enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand\getlongtablewidth{%
 \begingroup
  \ifcsname LT@\roman{LT@tables}\endcsname
  \global\longtablewidth=0pt
  \renewcommand\LT@entry[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}%
  \@nameuse{LT@\roman{LT@tables}}%
  \fi
\endgroup}


\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            pdfauthor={},
            pdftitle={The prevalence of statistical inconsistencies in management research: A replication of Nuijten et al. (2015)},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=black,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls

\setlength{\parindent}{0pt}
%\setlength{\parskip}{0pt plus 0pt minus 0pt}

\setlength{\emergencystretch}{3em}  % prevent overfull lines

\setcounter{secnumdepth}{0}
\ifxetex
  \usepackage{polyglossia}
  \setmainlanguage{}
\else
  \usepackage[english]{babel}
\fi

% Manuscript styling
\captionsetup{font=singlespacing,justification=justified}
\usepackage{csquotes}
\usepackage{upgreek}



\usepackage{tikz} % Variable definition to generate author note

% fix for \tightlist problem in pandoc 1.14
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% Essential manuscript parts
  \title{The prevalence of statistical inconsistencies in management research: A
replication of Nuijten et al. (2015)}

  \shorttitle{statistical inconsistencies}


  \author{Johannes M. van Zelst\textsuperscript{1}, Peter Kruyen\textsuperscript{2}, \& Chris H. J. Hartgerink\textsuperscript{3}}

  \def\affdep{{"", "", ""}}%
  \def\affcity{{"", "", ""}}%

  \affiliation{
    \vspace{0.5cm}
          \textsuperscript{1} Department of Organization Studies, Tilburg University\\
          \textsuperscript{2} Department of Public Administration, Radboud University Nijmegen\\
          \textsuperscript{3} Department of Methodology and Statistics, Tilburg University  }

 % If no author_note is defined give only author information if available
    \authornote{
    \newcounter{author}
                      Correspondence concerning this article should be addressed to Johannes M. van Zelst, Warandelaan 2, 5037 AB Tilburg, The Netherlands. E-mail: \href{mailto:j.m.vanzelst@uvt.nl}{\nolinkurl{j.m.vanzelst@uvt.nl}}
                                    }
  

  \abstract{This study documents reporting inconsistencies in a sample of over X
p-values reported in forty major management journals from 1995 until
2015, using the new R package statcheck. We find that X\% of the
articles in management research contains at least one reporting
inconsistency and X\% contains at least one gross inconsistency, which
might alter the conclusions. This corroborates/disagrees with findings
by Nuijten, Hartgerink, Assen, Epskamp, \& Wicherts (2015) who found
similar/dissimilar results.}
  



\begin{document}

\maketitle



\paragraph{Introduction}\label{introduction}

The use of null hypothesis significance testing is widespread in
management research (Lockett, McWilliams, \& {Van Fleet}, 2014;
Orlitzky, 2012; Schwab, Abrahamson, Starbuck, \& Fidler, 2011). The
advancement of science relies on correctly reported statistical results,
such as test statistics, degrees of freedom and p-values. However,
studies fields including psychology and medicine found that researchers
too often report statistical inconsistencies where some even lead to a
change in substantive conclusions (Bakker \& Wicherts, 2011;
Garc{í}a-Berthou \& Alcaraz, 2004; Nuijten et al., 2015). Recently,
management scholars started to document that some statistical findings
in their field might be inaccurate too. Goldfarb \& King (2016) found
that effect sizes in management research are inflated by around 24-40\%.
Management scholars also noted that \enquote{honest mistakes and
possible scientific misconduct pose a worrisome threat to the
trustworthiness of accumulated knowledge} (Bergh, Sharp, \& Li, 2016, p.
2). They note that statistical inconsistencies An assessment of the
prevalence of statistical inconsistencies in the field of management and
public administration is required to uphold the trustworthiness in our
results and theories. We therefore conduct a replication of Nuijten et
al. (2015) and investigate the prevalence of statistical inconsistencies
in 33 leading management and public administration journals over a time
period of twenty years.\\A statistical inconsistency can occur when
either the test statistic, the degrees of freedom or the resulting
p-value is erroneously reported. The misrepresentation of any of these
values may have severe consequences for conclusions that are drawn with
respect to the underlying theory. First, meta-analyses may produce
biased results when erroneous test statistics or degrees of freedom,
i.e.~sample size, are reported in studies and subsequently used as data
by meta-analysts. Second, when a p-value is interpreted as significant
while it is actually insignificant, this may lead to wrong substantive
conclusions. It is therefore important that authors, reviewers and
editors spot these inconsistencies whenever possible. A documentation of
the inconsistency rate in management and public administration is
therefore necessary if we are to uphold the trustworthiness of these
fields.\\The erroneous reporting of statistical results might be the
result of accidental inconsistencies or purposefully rounding off to a
statistically significant result. First, authors can mistakenly round of
a p-value (for example, reporting a p-value of 0.056 as p = 0.05) or
incorrectly report test statistics and/or sample sizes. For example, a
simple typographical error such as \emph{F}(2,56) = 1.203, p \textless{}
.001 instead of \emph{F}(2,56) = 12.03, p \textless{} .001 produces a
gross inconsistency, without the p-value being incorrect (Nuijten et
al., 2015, p. 10). On the other hand, authors sometimes also engage in
intentionally rounding of a p-value to make the result come across as
significant while it actually was insignificant. Banks et al. (2016)
find in a large-scale survey of management scholars that more than 10\%
of their respondents engaged in this form of questionable research
practice in at least one study. Given the serious consequences of
statistical inconsistencies, whether they are accidental or intentional,
we attempt to systematically document the prevalence of statistical
inconsistencies in the organizational sciences.\\Our contributions are
threefold. First, we document the prevalence of reporting
inconsistencies in management and public administration research. The
results can be directly compared to the study of Nuijten et al. (2015)
as we replicate their study using the same analyses. Second, we study
whether inconsistencies have increased over time and if they are related
to journal characteristics such as publisher and impact factor. We can
draw population-level conclusions as we use an automated procedure (the
R package statcheck, Epskamp \& Nuijten, 2015) which allows us to
extract test statistics from thousands of published articles. Last, we
offer a number of solutions that might help to partly solve the problem
of reporting errors for future research.

\paragraph{METHODS}\label{methods}

\emph{Sample}\\\hspace*{0.333em}\hspace*{0.333em}\hspace*{0.333em}\hspace*{0.333em}\hspace*{0.333em}\hspace*{0.333em}A
selection of articles was made by searching the flagship empirical
journals in management and public administration research. We used a
link spider and scraper to automatically download HTML articles from
top-tier journals that publish empirical papers. For our main results,
we excluded articles that were only available in PDF format as the
reliability of these results are lower given our automated procedure
(see below). We downloaded all articles that were available in HTML from
1995-2015 to provide a comprehensive and timely overview of reporting
errors. Stuk over hoe we precies gedownload hebben. We downloaded all
articles in an automated fashion, so the sample of downloaded articles
includes editorials, corrigendums and retractions. The final number of
articles that we downloaded is X. Table 1 contains the publisher per
journal, the number of articles that we downloaded per journal, and the
number of articles from which we were able to extract results.

\emph{Explaining statcheck}\\To document the prevalence of reporting
errors on a large-scale, we made use of an automated procedure in R (R
Core Team, 2016). Statcheck extracts statistical results and
recalculates p-values based on reported test statistics and their
degrees of freedom (Nuijten et al., 2015, p. 2). Statcheck executes the
procedure in four steps. First, statcheck converts the collection of PDF
or html files that are read by R into plain text files. PDF files are
more problematic to convert as some publishers use signs instead of
actual characters. Furthermore, some publishers place text in multiple
columns per page, where test statistics, degrees of freedom and the
associated p-value can be placed over multiple columns. The program is
unable to deal with these issues when PDF files are used, whereas html
files are rendered correctly.\\Second, statcheck extracts \emph{t},
\emph{F}, \emph{r}, \emph{Chi-Square}, and \emph{Z} statistics as well
as the accompanying degrees of freedom and p-value. As statcheck is an
automated procedure, it can only read prespecified strings of text. The
program is currently capable of reading results that are reported in APA
format, for example \emph{F}(1, 238) = 2.94, p = 0.088. Furthermore,
statcheck takes different spacing into account, and also reads results
that are reported as nonsignificant (ns) (Nuijten et al., 2015, p. 2).
This implies however that the program is currently unable to read
results that are reported in tables. Therefore, we are unable to assess
the prevalence of errors in tables that report, for example, regression
results. Third, statcheck recalculates a p-value on the basis of the
reported test statistics and the associated degrees of freedom. Tests
are assumed to be two-tailed by default, but the program also checks and
reports whether the text mentions one-tailed tests.\\Finally, statcheck
compares the reported p-value with the recalculated p-value and
indicates whether the reported p-value is consistent with the reported
test statistic and degrees of freedom or not. In case the recalculated
p-value and the reported value are not the same, the result is marked as
a reporting error. Sometimes, a reporting error might actually change
the conclusion of the associated test (under Î± = 0.05). Statcheck then
reports the result as a decision error. For example, the reported
p-value is p \textless{} 0.05 while the recalculated p-value is p =
0.052. As a robustness check, we also report the number of decision
errors when Î± is set to 0.10 (commonly regarded as marginal
significance in management and public administration research).
Statcheck subsequently scans the text for the words one-tailed,
one-sided, or directional. Results that were set to error/decision error
are marked as consistent if the article mentions one of these
words.\\The advantage of this automated procedure is that it allows us
to assess the prevalence of reporting inconsistencies on a very large
scale. Furthermore, the automated procedure eliminates human errors
which are bound to be made when results are recalculated by hand. The
disadvantage of an automated procedure is that it will miss statistical
tests that are not reported according to APA standards. Nuijten et al.
(2015) conducted an extensive validity check to assess whether statcheck
does not produce a biased assessments of the inconsistency rate. They
compared a subsample of the results of statcheck's analysis with a
manual check of the same p-values and found that the inter-rater
reliability for errors was 0.76 and for decision errors 0.89. Statcheck
slightly overestimated the prevalence of errors in this validity check,
so the results should be interpreted with care. For a detailed
discussion of the validity of statcheck, see Nuijten et al. (2015).

\emph{Analyses}\\The population of interest here is all APA-reported
test results in the selected top-tier journals in management and public
administration research. Our sample thus includes the entire population
of articles with results reported in APA-style, which is why we do not
use statistical tests in our analyses as we do not need to generalize
from our sample to a population. We report the prevalence of (gross)
inconsistencies per journal and explore linear trends over time. We also
compare whether gross inconsistencies are more likely for results that
are reported as statistically significant as compared to insignificant
results. Last, we analyze a number of journal-level covariates to
observe whether they influence the number of inconsistencies. We explore
whether there are differences in the percentage of inconsistencies
across different publishers: we included journals from general
publishers such as Wiley, Elsevier, and Sage as well as dedicated
publishers INFORMS, the Academy of Management, and one APA journal. We
also report regression results for the relationship between journal
impact factor and the amount of inconsistencies. Previous research has
found that journal impact factor and retraction frequency of articles
are positively related (Fang \& Casadevall, 2011). Therefore, it might
be interesting to explore whether journal impact factor and the
prevalence of statistical inconsistencies are related.

\emph{Robustness}\\Since many journals only publish HTML articles for
more recent years, we also downloaded all articles in PDF and used
statcheck on these articles. As explained above, the conversion to text
files is less reliable for PDFs than for HTML files. We therefore use
this robustness check as a sensitivity analysis and the results ought to
be interpreted with caution.

\paragraph*{References}\label{references}
\addcontentsline{toc}{paragraph}{References}

Bakker, M., \& Wicherts, J. M. (2011). The (mis)reporting of statistical
results in psychology journals. \emph{Behavior Research Methods},
\emph{43}(3), 666--678.
doi:\href{http://dx.doi.org/10.3758/s13428-011-0089-5}{10.3758/s13428-011-0089-5}

Banks, G. C., O'Boyle, E. H., Pollack, J. M., White, C. D., Batchelor,
J. H., Whelpley, C. E., \ldots{} Adkins, C. L. (2016). Questions about
questionable research practices in the field of management: A guest
commentary. \emph{Journal of Management}, \emph{42}(1), 5--20.
doi:\href{http://dx.doi.org/10.1177/0149206315619011}{10.1177/0149206315619011}

Bergh, D., Sharp, B., \& Li, M. (2016). Tests for identifying ``red
flags'' in empirical findings: Demonstration and recommendations for
authors, reviewers and editors. \emph{Academy of Management Learning \&
Education}.
doi:\href{http://dx.doi.org/10.5465/amle.2015.0406}{10.5465/amle.2015.0406}

Fang, F. C., \& Casadevall, A. (2011). Retracted Science and the
Retraction Index. \emph{Infection and Immunity}, \emph{79}(10),
3855--3859.
doi:\href{http://dx.doi.org/10.1128/IAI.05661-11}{10.1128/IAI.05661-11}

Garc{í}a-Berthou, E., \& Alcaraz, C. (2004). Incongruence between test
statistics and P values in medical papers. \emph{BMC Medical Research
Methodology}, \emph{4}(1), 13.
doi:\href{http://dx.doi.org/10.1186/1471-2288-4-13}{10.1186/1471-2288-4-13}

Goldfarb, B., \& King, A. A. (2016). Scientific apophenia in strategic
management research: Significance tests \& mistaken inference.
\emph{Strategic Management Journal}, \emph{37}(1), 167--176.
doi:\href{http://dx.doi.org/10.1002/smj.2459}{10.1002/smj.2459}

Lockett, A., McWilliams, A., \& {Van Fleet}, D. D. (2014). Reordering
Our Priorities by Putting Phenomena before Design: Escaping the
Straitjacket of Null Hypothesis Significance Testing. \emph{British
Journal of Management}, \emph{25}(4), 863--873.
doi:\href{http://dx.doi.org/10.1111/1467-8551.12063}{10.1111/1467-8551.12063}

Nuijten, M. B., Hartgerink, C. H. J., Assen, M. A. L. M. van, Epskamp,
S., \& Wicherts, J. M. (2015). The prevalence of statistical reporting
errors in psychology (1985--2013). \emph{Behavior Research Methods}.
doi:\href{http://dx.doi.org/10.3758/s13428-015-0664-2}{10.3758/s13428-015-0664-2}

Orlitzky, M. (2012). How can significance tests be deinstitutionalized?
\emph{Organizational Research Methods}, \emph{15}(2), 199--228.
doi:\href{http://dx.doi.org/10.1177/1094428111428356}{10.1177/1094428111428356}

Schwab, A., Abrahamson, E., Starbuck, W. H., \& Fidler, F. (2011).
Researchers Should Make Thoughtful Assessments Instead of
Null-Hypothesis Significance Tests. \emph{Organization Science},
\emph{22}(4), 1105--1120.
doi:\href{http://dx.doi.org/10.1287/orsc.1100.0557}{10.1287/orsc.1100.0557}





\end{document}
